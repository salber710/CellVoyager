import os
import pandas as pd
import json
import openai
from tqdm import tqdm
from dotenv import load_dotenv

NUM_RUN = 3
MODEL = "gpt-4o"

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI()

input_dir = "inputs"
output_dir = "outputs"

agent_output_path = os.path.join(output_dir, f"agent_analyses_conditioned_{MODEL.replace('-', '')}_{{index}}.json")
csv_path_1 = os.path.join(input_dir, "gemini_2.5_pro_qa_conditioned.csv")
csv_path_2 = os.path.join(input_dir, "gemini_2.5_pro_qa_conditioned_p2.csv")

df_1 = pd.read_csv(csv_path_1)
df_2 = pd.read_csv(csv_path_2)
df = pd.concat([df_1, df_2])

df['analyses_include'] = df['analyses_include'].apply(eval)
df['analyses_rest'] = df['analyses_rest'].apply(eval)


def parse_response(response):
    if isinstance(response, str):
        response = re.sub(r'[\x00-\x1F\x7F]', '', response)
        response = response.replace('```json', '').replace('```python', '').replace('```', '').strip()
        parsed = json.loads(response)
    elif isinstance(response, list):
        parsed = []
        for r in response:
            r = r.replace('```json', '').replace('```python', '').replace('```', '').strip()
            r = json.loads(r)
            parsed.append(r)
    return parsed

def get_response(prompt, system_prompt, use_json=False):
    if use_json:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )
        result = response.choices[0].message.content
        return json.loads(result)
    else:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
        )
        result = response.choices[0].message.content
        return result

analyses_overview = open(os.path.join("/home/groups/jamesz/salber/scAgent_v2", "prompts", "DeepResearch_Analyses.txt")).read()


first_draft_prompt = f"""
You will be provided the background/introduction from a research paper as well as a set of computational analyses from the research paper.
The rest of the computational analyses done in the paper are hidden from you.

Your role is to propose a computational analysis that you think was done in the hidden part of the paper.

Ensure that your output is in the specified JSON format.

For the analysis plan, think of the analysis plan as a scientific workflow:
    1. Start with exploratory data analysis that is broad and tests many things
    2. Then, focus on the more promising results from the exploratory phase by creating more focused analyses
    3. Include statistical validation of your results where appropiate
Do not number the analysis plan.
Each step in the analysis plan should be distinct from one another and could involve loading the data, conducting a statistical analysis, printing information about the AnnData object, etc.
Use however many steps is appropiate, but go for at least 5 steps. 


Ensure that your analyses solely use data explicitly mentioned in the paper. For example, if only RNA-seq data is mentioned, do NOT suggest spatial.
Likewise, if no spliced/unspliced RNA counts are mentioned, do NOT suggest RNA velocity.

Here are the previous analyses attempted:
{{past_analyses}}

Here is the background information from the paper and the subset of computational analyses:
{{paper_txt}}

For the rest of the prompt, we have examples of potential analyses:
{analyses_overview}
"""



system_prompt = f"""
You are a creative and skilled expert in single-cell transcriptomics computational analysis.

Output your response in the following JSON format (do not number the analysis steps, just list them):
{{
    "hypothesis": "...",
    "analysis_plan": ["First step", "Second step", ...],
    "summary": "A string describing the analysis in a detailed paragraph outlining how you will conduct the analysis"
}}
"""


critic_prompt = f"""
You will be given a hypothesis, analysis plan, and a summary of the analysis plan.
This analysis was generated by being given the background/introduction from a research paper (shown below) and a subset of the computational analyses in the research paper.
The rest of the computational analyses are hidden and the goal is to propose a computational analysis that is likely to be in that hidden set.

Your role is to provide feedback for the analysis based on these goals.

Ensure that the analyses solely uses data explicitly mentioned in the paper. For example, if only RNA-seq data is mentioned, the analysis should NOT involve spatial analyses.
Likewise, if no spliced/unspliced RNA counts are mentioend, the analysis should NOT suggest RNA velocity.

Analysis Summary:
{{summary}}

Analysis Hypothesis:
{{hypothesis}}

Analysis Plan:
{{analysis_plan}}

Here is the background information from the paper and the subset of computational analyses:
{{paper_txt}}

Previous Analysis Attempted:
{{past_analyses}}

For the rest of the prompt, we have examples of potential analyses:
{analyses_overview}
"""

incorporate_prompt = f"""
You will be given a hypothesis, analysis plan, and a summary of the analysis plan.
This analysis was generated by being given the background/introduction from a research paper (shown below) and a subset of the computational analyses in the research paper.
The rest of the computational analyses are hidden and the goal is to propose a computational analysis that is likely to be in that hidden set.

You will also be given feedback for the analysis in order so that it achieves these goals. 
Your role is to incorporate that feedback and update the analysis components (summary, hypothesis, analysis plan)

Analysis Summary:
{{summary}}

Analysis Hypothesis:
{{hypothesis}}

Analysis Plan:
{{analysis_plan}}

Feedback:
{{feedback}}

Here are the previous analyses attempted:
{{past_analyses}}

Here is the background information from the paper and the subset of computational analyses:
{{paper_txt}}

For the rest of the prompt, we have examples of potential analyses:
{analyses_overview}
"""



###### RUN AGENT ######
for i in range(NUM_RUN):
    print(f"Running {i+1}/{NUM_RUN}")
    index = i + 1
    agent_output_path_run = agent_output_path.format(index=index)
    if os.path.exists(agent_output_path_run):
        continue
    all_agent_analyses = []
    for i, row in df.iterrows():
        context = row['context']
        analyses_rest_paper = row['analyses_rest']
        analyses_include_paper = row['analyses_include']

        for index, analysis in enumerate(analyses_include_paper):
            analysis_description = analysis['description']
            context += f"\nPaper Analysis {index + 1}:\n{analysis_description}"

        past_analyses = ""
        for analysis in analyses_include_paper:
            past_analyses += f"{analysis}\n\n"

        agent_analyses = []
        print(f"Running {len(analyses_rest_paper)} analyses for paper {i+1}")
        for j in range(len(analyses_rest_paper)):
            print(f"Running {j+1}/{len(analyses_rest_paper)}")
            first_draft_prompt_filled = first_draft_prompt.format(past_analyses=past_analyses, paper_txt=context)
            first_draft = get_response(first_draft_prompt_filled, system_prompt, use_json=True)

            hypothesis, analysis_plan, summary = first_draft["hypothesis"], first_draft["analysis_plan"], first_draft["summary"]
            critic_prompt_filled = critic_prompt.format(summary=summary, hypothesis=hypothesis, analysis_plan=analysis_plan, 
                                                    paper_txt=context, past_analyses=past_analyses)
            feedback = get_response(critic_prompt_filled, "You are a single-cell bioinformatics expert providing feedback on code and analysis plan.", use_json=False)

            incorporate_prompt_filled = incorporate_prompt.format(summary=summary, hypothesis=hypothesis, analysis_plan=analysis_plan, 
                                                                    feedback=feedback, past_analyses=past_analyses, paper_txt=context)
            final_analysis = get_response(incorporate_prompt_filled, system_prompt, use_json=True)

            analysis = final_analysis['summary']
            agent_analyses.append(analysis)
            past_analyses += f"{analysis}\n\n"

        all_agent_analyses.append(agent_analyses)

        # Save intermediate result after each iteration
        with open(agent_output_path, "w") as f:
            json.dump(all_agent_analyses, f, indent=2)


###### EVALUATE AGENT ######
analyses_rest = df['analyses_rest'].tolist()
micro_averages, macro_averages = [], []
for i in range(NUM_RUN):
    index = i + 1
    agent_output_path_run = agent_output_path.format(index=index)
    with open(agent_output_path_run, "r") as f:
        all_agent_analyses = json.load(f)
    
    all_judge_responses = []
    for i, agent_analysis_list in enumerate(all_agent_analyses):
        print(i)
        judge_responses = []
        for j, agent_analysis in enumerate(agent_analysis_list):
            prompt = f"""
            You are given a proposed analysis for single-cell transcriptomics and a set of ground truth analyses. Your task is to determine whether the proposed analysis matches at least one analysis from the set of ground truth analyses.

            Proposed:
            {agent_analysis}

            Ground truth:
            {analyses_rest[i]}

            Give your answer in the following format:
            {{
                "match": true/false,
                "reason": "explanation of the match or mismatch and if match, which analysis it matches"
            }}
            """
            response = client.responses.create(
                model="gpt-4o",
                input=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "input_text",
                                "text": prompt,
                            },
                        ]
                    }
                ]
            )

            response = response.output_text
            judge_responses.append(response)
        all_judge_responses.append(judge_responses)

    df = pd.DataFrame({'raw_judge_response': all_judge_responses})
    df['parsed_judge_response'] = df['raw_judge_response'].apply(parse_response)
    df['num_match'] = df['parsed_judge_response'].apply(lambda x: sum(map(lambda y: int(y['match']), x)))
    num_per_paper = df['parsed_judge_response'].apply(len)
    micro_avg = (df['num_match'] / num_per_paper).mean()
    macro_avg = df['num_match'].sum() / num_per_paper.sum()

    micro_averages.append(micro_avg)
    macro_averages.append(macro_avg)




#### SAVE RESULTS #####
with open(os.path.join(home_dir, "results", f"agent_grades_conditioned_{MODEL}.pkl"), "wb") as f:
    pickle.dump({
        "micro_averages": micro_averages,
        "macro_averages": macro_averages,
        "micro_avg": sum(micro_averages) / len(micro_averages),
        "micro_std": np.std(micro_averages),
        "macro_avg": sum(macro_averages) / len(macro_averages), 
        "macro_std": np.std(macro_averages)
    }, f)
