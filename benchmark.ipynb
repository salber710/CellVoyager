{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/samuelalber/Documents/Research/jzou/salber/scAgent_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "#from notebook_generator import generate_notebook\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_output_path = \"agent_analyses_unconditioned_gpt4o_p2.json\"\n",
    "csv_path_1 = os.path.join(home_dir, \"gemini_2.5_pro_qa_conditioned.csv\")\n",
    "csv_path_2 = os.path.join(home_dir, \"gemini_2.5_pro_qa_conditioned_p2.csv\")\n",
    "df_1 = pd.read_csv(csv_path_1)\n",
    "df_2 = pd.read_csv(csv_path_2)\n",
    "df = pd.concat([df_1, df_2])\n",
    "\n",
    "df['analyses_full'] = df['analyses_full'].apply(eval)\n",
    "analyses_full = df['analyses_full'].tolist()\n",
    "home_dir = \"/Users/samuelalber/Documents/Research/jzou/salber/scAgent_v2\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response):\n",
    "    if isinstance(response, str):\n",
    "        response = re.sub(r'[\\x00-\\x1F\\x7F]', '', response)\n",
    "        response = response.replace('```json', '').replace('```python', '').replace('```', '').strip()\n",
    "        parsed = json.loads(response)\n",
    "    elif isinstance(response, list):\n",
    "        parsed = []\n",
    "        for r in response:\n",
    "            r = r.replace('```json', '').replace('```python', '').replace('```', '').strip()\n",
    "            r = json.loads(r)\n",
    "            parsed.append(r)\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_overview = open(os.path.join(home_dir, \"prompts\", \"DeepResearch_Analyses.txt\")).read()\n",
    "\n",
    "\n",
    "first_draft_prompt = f\"\"\"\n",
    "You will be provided the background/introduction from a research paper.\n",
    "The computational analyses done in the paper are hidden from you.\n",
    "\n",
    "Your role is to propose a computational analysis that you think was most likely done in the paper.\n",
    "\n",
    "Ensure that your output is in the specified JSON format.\n",
    "\n",
    "For the analysis plan, think of the analysis plan as a scientific workflow:\n",
    "    1. Start with exploratory data analysis that is broad and tests many things\n",
    "    2. Then, focus on the more promising results from the exploratory phase by creating more focused analyses\n",
    "    3. Include statistical validation of your results where appropiate\n",
    "Do not number the analysis plan.\n",
    "Each step in the analysis plan should be distinct from one another and could involve loading the data, conducting a statistical analysis, printing information about the AnnData object, etc.\n",
    "Use however many steps is appropiate, but go for at least 5 steps. \n",
    "\n",
    "\n",
    "Ensure that your analyses solely use data explicitly mentioned in the paper. For example, if only RNA-seq data is mentioned, do NOT suggest spatial.\n",
    "Likewise, if no spliced/unspliced RNA counts are mentioned, do NOT suggest RNA velocity.\n",
    "\n",
    "Here are the previous analyses attempted:\n",
    "{{past_analyses}}\n",
    "\n",
    "Here is the background information from the paper:\n",
    "{{paper_txt}}\n",
    "\n",
    "For the rest of the prompt, we have examples of potential analyses:\n",
    "{analyses_overview}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a creative and skilled expert in single-cell transcriptomics computational analysis.\n",
    "\n",
    "Output your response in the following JSON format (do not number the analysis steps, just list them):\n",
    "{{\n",
    "    \"hypothesis\": \"...\",\n",
    "    \"analysis_plan\": [\"First step\", \"Second step\", ...],\n",
    "    \"summary\": \"A string describing the analysis in a detailed paragraph outlining how you will conduct the analysis\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "critic_prompt = f\"\"\"\n",
    "You will be given a hypothesis, analysis plan, and a summary of the analysis plan.\n",
    "This analysis was generated by being given the background/introduction from a research paper (shown below).\n",
    "The computational analyses done in the paper are hidden and the goal is to propose a computational analysis that is most likely to be in that hidden set.\n",
    "\n",
    "Your role is to provide feedback for the analysis based on these goals.\n",
    "\n",
    "Ensure that the analyses solely uses data explicitly mentioned in the paper. For example, if only RNA-seq data is mentioned, the analysis should NOT involve spatial analyses.\n",
    "Likewise, if no spliced/unspliced RNA counts are mentioend, the analysis should NOT suggest RNA velocity.\n",
    "\n",
    "Analysis Summary:\n",
    "{{summary}}\n",
    "\n",
    "Analysis Hypothesis:\n",
    "{{hypothesis}}\n",
    "\n",
    "Analysis Plan:\n",
    "{{analysis_plan}}\n",
    "\n",
    "Here is the background information from the paper:\n",
    "{{paper_txt}}\n",
    "\n",
    "Previous Analysis Attempted:\n",
    "{{past_analyses}}\n",
    "\n",
    "For the rest of the prompt, we have examples of potential analyses:\n",
    "{analyses_overview}\n",
    "\"\"\"\n",
    "\n",
    "incorporate_prompt = f\"\"\"\n",
    "You will be given a hypothesis, analysis plan, and a summary of the analysis plan.\n",
    "This analysis was generated by being given the background/introduction from a research paper (shown below).\n",
    "The computational analyses done in the paper are hidden and the goal is to propose a computational analysis that is most likely to be in that hidden set.\n",
    "\n",
    "You will also be given feedback for the analysis in order so that it achieves these goals. \n",
    "Your role is to incorporate that feedback and update the analysis components (summary, hypothesis, analysis plan)\n",
    "\n",
    "Analysis Summary:\n",
    "{{summary}}\n",
    "\n",
    "Analysis Hypothesis:\n",
    "{{hypothesis}}\n",
    "\n",
    "Analysis Plan:\n",
    "{{analysis_plan}}\n",
    "\n",
    "Feedback:\n",
    "{{feedback}}\n",
    "\n",
    "Here are the previous analyses attempted:\n",
    "{{past_analyses}}\n",
    "\n",
    "Here is the background information from the paper:\n",
    "{{paper_txt}}\n",
    "\n",
    "For the rest of the prompt, we have examples of potential analyses:\n",
    "{analyses_overview}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, system_prompt, use_json=False):\n",
    "    if use_json:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"o3-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        return json.loads(result)\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"o3-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 7 analyses for paper 1\n",
      "Running 1/7\n",
      "Running 2/7\n",
      "Running 3/7\n",
      "Running 4/7\n",
      "Running 5/7\n",
      "Running 6/7\n",
      "Running 7/7\n",
      "Running 6 analyses for paper 2\n",
      "Running 1/6\n",
      "Running 2/6\n",
      "Running 3/6\n",
      "Running 4/6\n",
      "Running 5/6\n",
      "Running 6/6\n",
      "Running 11 analyses for paper 3\n",
      "Running 1/11\n",
      "Running 2/11\n",
      "Running 3/11\n",
      "Running 4/11\n",
      "Running 5/11\n",
      "Running 6/11\n",
      "Running 7/11\n",
      "Running 8/11\n",
      "Running 9/11\n",
      "Running 10/11\n",
      "Running 11/11\n",
      "Running 10 analyses for paper 4\n",
      "Running 1/10\n",
      "Running 2/10\n",
      "Running 3/10\n",
      "Running 4/10\n",
      "Running 5/10\n",
      "Running 6/10\n",
      "Running 7/10\n",
      "Running 8/10\n",
      "Running 9/10\n",
      "Running 10/10\n",
      "Running 9 analyses for paper 5\n",
      "Running 1/9\n",
      "Running 2/9\n",
      "Running 3/9\n",
      "Running 4/9\n",
      "Running 5/9\n",
      "Running 6/9\n",
      "Running 7/9\n",
      "Running 8/9\n",
      "Running 9/9\n",
      "Running 16 analyses for paper 6\n",
      "Running 1/16\n",
      "Running 2/16\n",
      "Running 3/16\n",
      "Running 4/16\n",
      "Running 5/16\n",
      "Running 6/16\n",
      "Running 7/16\n",
      "Running 8/16\n",
      "Running 9/16\n",
      "Running 10/16\n",
      "Running 11/16\n",
      "Running 12/16\n",
      "Running 13/16\n",
      "Running 14/16\n",
      "Running 15/16\n",
      "Running 16/16\n",
      "Running 12 analyses for paper 7\n",
      "Running 1/12\n",
      "Running 2/12\n",
      "Running 3/12\n",
      "Running 4/12\n",
      "Running 5/12\n",
      "Running 6/12\n",
      "Running 7/12\n",
      "Running 8/12\n",
      "Running 9/12\n",
      "Running 10/12\n",
      "Running 11/12\n",
      "Running 12/12\n",
      "Running 8 analyses for paper 8\n",
      "Running 1/8\n",
      "Running 2/8\n",
      "Running 3/8\n",
      "Running 4/8\n",
      "Running 5/8\n",
      "Running 6/8\n",
      "Running 7/8\n",
      "Running 8/8\n",
      "Running 7 analyses for paper 9\n",
      "Running 1/7\n",
      "Running 2/7\n",
      "Running 3/7\n",
      "Running 4/7\n",
      "Running 5/7\n",
      "Running 6/7\n",
      "Running 7/7\n",
      "Running 10 analyses for paper 10\n",
      "Running 1/10\n",
      "Running 2/10\n",
      "Running 3/10\n",
      "Running 4/10\n",
      "Running 5/10\n",
      "Running 6/10\n",
      "Running 7/10\n",
      "Running 8/10\n",
      "Running 9/10\n",
      "Running 10/10\n"
     ]
    }
   ],
   "source": [
    "NUM_RUN = 5\n",
    "for i in range(NUM_RUN):\n",
    "    index = i + 1\n",
    "    agent_output_path = os.path.join(home_dir, f\"agent_analyses_unconditioned_gpt4o_{index}.json\")\n",
    "    all_agent_analyses = []\n",
    "    for i, row in df.iterrows():\n",
    "        context = row['context']\n",
    "        analyses_full_paper = row['analyses_full']\n",
    "        # Save context to temporary file\n",
    "        temp_file = \"temp_cond.txt\"\n",
    "        with open(temp_file, \"w\") as f:\n",
    "            f.write(context)\n",
    "\n",
    "        past_analyses = \"\"\n",
    "\n",
    "        agent_analyses = []\n",
    "        print(f\"Running {len(analyses_full_paper)} analyses for paper {i+1}\")\n",
    "        for j in range(len(analyses_full_paper)):\n",
    "            print(f\"Running {j+1}/{len(analyses_full_paper)}\")\n",
    "            first_draft_prompt_filled = first_draft_prompt.format(past_analyses=past_analyses, paper_txt=context)\n",
    "            first_draft = get_response(first_draft_prompt_filled, system_prompt, use_json=True)\n",
    "\n",
    "            hypothesis, analysis_plan, summary = first_draft[\"hypothesis\"], first_draft[\"analysis_plan\"], first_draft[\"summary\"]\n",
    "            critic_prompt_filled = critic_prompt.format(summary=summary, hypothesis=hypothesis, analysis_plan=analysis_plan, \n",
    "                                                    paper_txt=context, past_analyses=past_analyses)\n",
    "            feedback = get_response(critic_prompt_filled, \"You are a single-cell bioinformatics expert providing feedback on code and analysis plan.\", use_json=False)\n",
    "\n",
    "            incorporate_prompt_filled = incorporate_prompt.format(summary=summary, hypothesis=hypothesis, analysis_plan=analysis_plan, \n",
    "                                                                    feedback=feedback, past_analyses=past_analyses, paper_txt=context)\n",
    "            final_analysis = get_response(incorporate_prompt_filled, system_prompt, use_json=True)\n",
    "\n",
    "            analysis = final_analysis['summary']\n",
    "            agent_analyses.append(analysis)\n",
    "            past_analyses += f\"{analysis}\\n\\n\"\n",
    "\n",
    "\n",
    "        all_agent_analyses.append(agent_analyses)\n",
    "\n",
    "        # Save intermediate result after each iteration\n",
    "        with open(agent_output_path, \"w\") as f:\n",
    "            json.dump(all_agent_analyses, f, indent=2)\n",
    "\n",
    "        # Delete temporary file\n",
    "        os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "with open(agent_output_path, \"r\") as f:\n",
    "    all_agent_analyses = json.load(f)\n",
    "all_judge_responses = []\n",
    "for i, agent_analysis_list in enumerate(all_agent_analyses):\n",
    "    judge_responses = []\n",
    "    for j, agent_analysis in enumerate(agent_analysis_list):\n",
    "        prompt = f\"\"\"\n",
    "        You are given a proposed analysis for single-cell transcriptomics and a set of ground truth analyses. Your task is to determine whether the proposed analysis matches at least one analysis from the set of ground truth analyses.\n",
    "\n",
    "        Proposed:\n",
    "        {agent_analysis}\n",
    "\n",
    "        Ground truth:\n",
    "        {analyses_full[i]}\n",
    "\n",
    "        Give your answer in the following format:\n",
    "        {{\n",
    "            \"match\": true/false,\n",
    "            \"reason\": \"explanation of the match or mismatch and if match, which analysis it matches\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4o\",\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"input_text\",\n",
    "                            \"text\": prompt,\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        response = response.output_text\n",
    "        judge_responses.append(response)\n",
    "    all_judge_responses.append(judge_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = pd.DataFrame({'raw_judge_response': all_judge_responses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_response(response):\n",
    "    def clean_and_parse(r):\n",
    "        # Remove markdown fences and control characters\n",
    "        r = re.sub(r'```(?:json|python)?', '', r, flags=re.IGNORECASE).strip()\n",
    "        r = re.sub(r'[\\x00-\\x1F\\x7F]', '', r)\n",
    "        return json.loads(r)\n",
    "    \n",
    "    if isinstance(response, str):\n",
    "        return clean_and_parse(response)\n",
    "    elif isinstance(response, list):\n",
    "        return [clean_and_parse(r) for r in response]\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported input type for parse_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed_judge_response'] = df['raw_judge_response'].apply(parse_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7193416305916306, 0.75)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num_match'] = df['parsed_judge_response'].apply(lambda x: sum(map(lambda y: int(y['match']), x)))\n",
    "num_per_paper = df['parsed_judge_response'].apply(len)\n",
    "micro_avg = (df['num_match'] / num_per_paper).mean()\n",
    "macro_avg = df['num_match'].sum() / num_per_paper.sum()\n",
    "micro_avg, macro_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6115911865911866, 0.6813186813186813)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num_match'] = df['parsed_judge_response'].apply(lambda x: sum(map(lambda y: int(y['match']), x)))\n",
    "num_per_paper = df['parsed_judge_response'].apply(len)\n",
    "micro_avg = (df['num_match'] / num_per_paper).mean()\n",
    "macro_avg = df['num_match'].sum() / num_per_paper.sum()\n",
    "micro_avg, macro_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4628110778110778, 0.42857142857142855)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num_match'] = df['parsed_judge_response'].apply(lambda x: sum(map(lambda y: int(y['match']), x)))\n",
    "num_per_paper = df['parsed_judge_response'].apply(len)\n",
    "micro_avg = (df['num_match'] / num_per_paper).mean()\n",
    "macro_avg = df['num_match'].sum() / num_per_paper.sum()\n",
    "micro_avg, macro_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4142223054723055, 0.42857142857142855)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num_match'] = df['parsed_judge_response'].apply(lambda x: sum(map(lambda y: int(y['match']), x)))\n",
    "num_per_paper = df['parsed_judge_response'].apply(len)\n",
    "micro_avg = (df['num_match'] / num_per_paper).mean()\n",
    "macro_avg = df['num_match'].sum() / num_per_paper.sum()\n",
    "micro_avg, macro_avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "single-cell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
